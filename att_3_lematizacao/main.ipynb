{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:14.751341400Z",
     "start_time": "2023-09-25T03:41:14.731823400Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a27f7cc1afa590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.084126Z",
     "start_time": "2023-09-25T03:41:14.734341700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6abd54a0d64e01d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.091156700Z",
     "start_time": "2023-09-25T03:41:15.084126Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package mac_morpho to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package mac_morpho is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('mac_morpho')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "decfa4e1d7835a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.099699400Z",
     "start_time": "2023-09-25T03:41:15.088156300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenizar(text):\n",
    "    vetor_token = sent_tokenize(text, language='portuguese')\n",
    "    total_tokens = []\n",
    "    for sent in vetor_token:\n",
    "        for token in word_tokenize(sent,language='portuguese'):\n",
    "            total_tokens.append(token)\n",
    "    return total_tokens\n",
    "\n",
    "def remover_stopwords(texto):\n",
    "    # Carregue a lista de stopwords da língua desejada (exemplo: português)\n",
    "    stopwords_lista = nltk.corpus.stopwords.words('portuguese')\n",
    "    \n",
    "    # Tokenize o texto em palavras\n",
    "    palavras = word_tokenize(texto)\n",
    "    \n",
    "    # Crie uma lista de palavras sem as stopwords\n",
    "    palavras_sem_stopwords = [palavra for palavra in palavras if palavra.lower() not in stopwords_lista]\n",
    "    \n",
    "    # Recrie o texto sem as stopwords\n",
    "    texto_sem_stopwords = ' '.join(palavras_sem_stopwords)\n",
    "    \n",
    "    return texto_sem_stopwords\n",
    "\n",
    "def remocao_caracteres_especiais(text):\n",
    "    padrao = r'[^\\w\\s]'\n",
    "    return re.sub(padrao, '', text)\n",
    "\n",
    "def contar_tipos_e_tokens(texto):\n",
    "    \n",
    "    contador_tipos = Counter(texto)\n",
    "    ranking = contador_tipos.most_common(20)\n",
    "\n",
    "    return ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "142fc5f89e953483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.108524600Z",
     "start_time": "2023-09-25T03:41:15.100701Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"capivara-pt.txt\", 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#print(tokenizar(remover_stopwords(remocao_caracteres_especiais(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81688e794934a3b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2676c87619823e3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.149038200Z",
     "start_time": "2023-09-25T03:41:15.103525500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text_n = remover_stopwords(remocao_caracteres_especiais(text))\n",
    "text_lemma = []\n",
    "for token in nlp(text_n):\n",
    "    text_lemma.append(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b44a66da9cb8bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c1cbe50a2a9e687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.160039100Z",
     "start_time": "2023-09-25T03:41:15.135039800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "text_stemmer = []\n",
    "for token in nlp(text_n):\n",
    "    text_stemmer.append(stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2bf582b8b4682",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Faça um ranking dos 20 lemas e das 20 raízes mais frequentes no texto, novamente mostrando a frequência de cada um deles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58bbf644ca622ef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.160039100Z",
     "start_time": "2023-09-25T03:41:15.153039200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking dos 20 Radiais mais frequentes no texto:\n",
      "1 - Radical: roed, Frequência: 3\n",
      "2 - Radical: famíl, Frequência: 2\n",
      "3 - Radical: classific, Frequência: 2\n",
      "4 - Radical: ocorr, Frequência: 2\n",
      "5 - Radical: m, Frequência: 2\n",
      "6 - Radical: adapt, Frequência: 2\n",
      "7 - Radical: pod, Frequência: 2\n",
      "8 - Radical: pes, Frequência: 2\n",
      "9 - Radical: kg, Frequência: 2\n",
      "10 - Radical: 12, Frequência: 2\n",
      "11 - Radical: sex, Frequência: 2\n",
      "12 - Radical: 15, Frequência: 2\n",
      "13 - Radical: ano, Frequência: 2\n",
      "14 - Radical: idad, Frequência: 2\n",
      "15 - Radical: capiv, Frequência: 1\n",
      "16 - Radical: nom, Frequência: 1\n",
      "17 - Radical: científ, Frequência: 1\n",
      "18 - Radical: hydrochoeru, Frequência: 1\n",
      "19 - Radical: hydrochaeril, Frequência: 1\n",
      "20 - Radical: espéci, Frequência: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ranking = contar_tipos_e_tokens(text_stemmer)\n",
    "\n",
    "# Exibir o ranking dos 20 tipos mais frequentes\n",
    "print(\"Ranking dos 20 Radiais mais frequentes no texto:\")\n",
    "cont = 1\n",
    "for tipo, frequencia in ranking:\n",
    "    print(f\"{cont} - Radical: {tipo}, Frequência: {frequencia}\")\n",
    "    cont+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b402c56a5ea915b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.169643600Z",
     "start_time": "2023-09-25T03:41:15.157039300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking dos 20 Lemas mais frequentes no texto:\n",
      "1 - Lema: roedor, Frequência: 3\n",
      "2 - Lema: família, Frequência: 2\n",
      "3 - Lema: ocorrer, Frequência: 2\n",
      "4 - Lema: m, Frequência: 2\n",
      "5 - Lema: poder, Frequência: 2\n",
      "6 - Lema: pesar, Frequência: 2\n",
      "7 - Lema: kg, Frequência: 2\n",
      "8 - Lema: 12, Frequência: 2\n",
      "9 - Lema: sexual, Frequência: 2\n",
      "10 - Lema: 15, Frequência: 2\n",
      "11 - Lema: ano, Frequência: 2\n",
      "12 - Lema: idade, Frequência: 2\n",
      "13 - Lema: capivar, Frequência: 1\n",
      "14 - Lema: nome, Frequência: 1\n",
      "15 - Lema: científico, Frequência: 1\n",
      "16 - Lema: hydrochoerus, Frequência: 1\n",
      "17 - Lema: hydrochaeris, Frequência: 1\n",
      "18 - Lema: espécie, Frequência: 1\n",
      "19 - Lema: mamífero, Frequência: 1\n",
      "20 - Lema: caviidae, Frequência: 1\n"
     ]
    }
   ],
   "source": [
    "ranking = contar_tipos_e_tokens(text_lemma)\n",
    "\n",
    "# Exibir o ranking dos 20 tipos mais frequentes\n",
    "print(\"Ranking dos 20 Lemas mais frequentes no texto:\")\n",
    "cont= 1\n",
    "for tipo, frequencia in ranking:\n",
    "    print(f\"{cont} - Lema: {tipo}, Frequência: {frequencia}\")\n",
    "    cont+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4944c70a48bd3c0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44c05cffcbf7a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 5.  Escrever o  pseudo códigode um dos  algoritmo de cálculo da distancia minima de edicao:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37a01b4b804239",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "a) Distância de Levensteien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a57668691aa09504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.170644800Z",
     "start_time": "2023-09-25T03:41:15.166118600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A distância de Levenshtein entre 'Mario' e 'Maria' é 1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein_distance(str1, str2):\n",
    "    len_str1 = len(str1)\n",
    "    len_str2 = len(str2)\n",
    "\n",
    "    # Inicializa uma matriz para armazenar as distâncias parciais\n",
    "    dp = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]\n",
    "    \n",
    "    for i in range(len_str1 + 1):\n",
    "        dp[i][0] = i\n",
    "\n",
    "    for j in range(len_str2 + 1):\n",
    "        dp[0][j] = j\n",
    "        \n",
    "    for i in range(1, len_str1 + 1):\n",
    "        for j in range(1, len_str2 + 1):\n",
    "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    return dp[len_str1][len_str2]\n",
    "\n",
    "str1 = \"Mario\"\n",
    "str2 = \"Maria\"\n",
    "distance = levenshtein_distance(str1, str2)\n",
    "print(f\"A distância de Levenshtein entre '{str1}' e '{str2}' é {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4beb21594ae7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "b) Distância de Needelemna-Wunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89684d3ff5c7431a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.180645500Z",
     "start_time": "2023-09-25T03:41:15.171644400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pontuação de Needleman-Wunsch entre 'Mario' e 'Maria' é 7\n"
     ]
    }
   ],
   "source": [
    "def needleman_wunsch(seq1, seq2, match_score=2, mismatch_score=-1, gap_penalty=-2):\n",
    "    len_seq1 = len(seq1)\n",
    "    len_seq2 = len(seq2)\n",
    "\n",
    "    # Inicializa a matriz de pontuações\n",
    "    dp = [[0] * (len_seq2 + 1) for _ in range(len_seq1 + 1)]\n",
    "\n",
    "    for i in range(1, len_seq1 + 1):\n",
    "        dp[i][0] = dp[i - 1][0] + gap_penalty\n",
    "\n",
    "    for j in range(1, len_seq2 + 1):\n",
    "        dp[0][j] = dp[0][j - 1] + gap_penalty\n",
    "\n",
    "    # Preenche a matriz de pontuações usando a fórmula de Needleman-Wunsch\n",
    "    for i in range(1, len_seq1 + 1):\n",
    "        for j in range(1, len_seq2 + 1):\n",
    "            if seq1[i - 1] == seq2[j - 1]:\n",
    "                match = dp[i - 1][j - 1] + match_score\n",
    "            else:\n",
    "                match = dp[i - 1][j - 1] + mismatch_score\n",
    "\n",
    "            gap1 = dp[i - 1][j] + gap_penalty\n",
    "            gap2 = dp[i][j - 1] + gap_penalty\n",
    "\n",
    "            dp[i][j] = max(match, gap1, gap2)\n",
    "\n",
    "    alignment_score = dp[len_seq1][len_seq2]\n",
    "\n",
    "    return alignment_score\n",
    "\n",
    "seq1 = \"Mario\"\n",
    "seq2 = \"Maria\"\n",
    "score = needleman_wunsch(seq1, seq2)\n",
    "print(f\"A pontuação de Needleman-Wunsch entre '{seq1}' e '{seq2}' é {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64808e265b2dcfb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "C) Distância de Smith-Waterman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96dbda4b4e7baf48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T03:41:15.253670200Z",
     "start_time": "2023-09-25T03:41:15.179644100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pontuação de Smith-Waterman entre 'Mario' e 'Maria' é 8\n",
      "Sequência alinhada 1: Mari\n",
      "Sequência alinhada 2: Mari\n"
     ]
    }
   ],
   "source": [
    "def smith_waterman(seq1, seq2, match_score=2, mismatch_score=-1, gap_penalty=-2):\n",
    "    len_seq1 = len(seq1)\n",
    "    len_seq2 = len(seq2)\n",
    "\n",
    "    # Inicializa a matriz de pontuações\n",
    "    dp = [[0] * (len_seq2 + 1) for _ in range(len_seq1 + 1)]\n",
    "    \n",
    "    max_score = 0 \n",
    "    max_i, max_j = 0, 0  \n",
    "\n",
    "    for i in range(1, len_seq1 + 1):\n",
    "        for j in range(1, len_seq2 + 1):\n",
    "            if seq1[i - 1] == seq2[j - 1]:\n",
    "                match = dp[i - 1][j - 1] + match_score\n",
    "            else:\n",
    "                match = dp[i - 1][j - 1] + mismatch_score\n",
    "\n",
    "            gap1 = dp[i - 1][j] + gap_penalty\n",
    "            gap2 = dp[i][j - 1] + gap_penalty\n",
    "\n",
    "            dp[i][j] = max(0, match, gap1, gap2)  # Impede pontuações negativas\n",
    "\n",
    "            # Mantém o controle da pontuação máxima encontrada\n",
    "            if dp[i][j] > max_score:\n",
    "                max_score = dp[i][j]\n",
    "                max_i, max_j = i, j\n",
    "\n",
    "    aligned_seq1 = []\n",
    "    aligned_seq2 = []\n",
    "    \n",
    "    i, j = max_i, max_j\n",
    "    while dp[i][j] > 0:\n",
    "        if dp[i][j] == dp[i - 1][j - 1] + (match_score if seq1[i - 1] == seq2[j - 1] else mismatch_score):\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif dp[i][j] == dp[i - 1][j] + gap_penalty:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append('-')\n",
    "            i -= 1\n",
    "        else:\n",
    "            aligned_seq1.append('-')\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            j -= 1\n",
    "    \n",
    "    aligned_seq1 = ''.join(reversed(aligned_seq1))\n",
    "    aligned_seq2 = ''.join(reversed(aligned_seq2))\n",
    "\n",
    "    return max_score, aligned_seq1, aligned_seq2\n",
    "\n",
    "seq1 = \"Mario\"\n",
    "seq2 = \"Maria\"\n",
    "score, aligned_seq1, aligned_seq2 = smith_waterman(seq1, seq2)\n",
    "print(f\"A pontuação de Smith-Waterman entre '{seq1}' e '{seq2}' é {score}\")\n",
    "print(f\"Sequência alinhada 1: {aligned_seq1}\")\n",
    "print(f\"Sequência alinhada 2: {aligned_seq2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
