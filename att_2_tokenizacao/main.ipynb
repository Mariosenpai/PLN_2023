{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:56:24.209090300Z",
     "start_time": "2023-09-16T22:56:24.200058300Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package mac_morpho to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package mac_morpho is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('mac_morpho')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:56:25.289326100Z",
     "start_time": "2023-09-16T22:56:25.279838700Z"
    }
   },
   "id": "74f331a3e4a38546"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenização"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b30ad735932f517a"
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "def tokenizar(text):\n",
    "    vetor_token = sent_tokenize(text, language='portuguese')\n",
    "    total_tokens = []\n",
    "    for sent in vetor_token:\n",
    "        for token in word_tokenize(sent,language='portuguese'):\n",
    "            total_tokens.append(token)\n",
    "    return total_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:58:10.833975800Z",
     "start_time": "2023-09-16T22:58:10.825162100Z"
    }
   },
   "id": "c69ce97acf24e0d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saída esperada: Uma lista que contém strings representando os tokens extraídos."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23ded57dfc9d25b4"
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "data": {
      "text/plain": "[('a', 'DT'),\n ('capivara', 'NN'),\n ('(', '('),\n ('nome', 'JJ'),\n ('científico', 'NN'),\n (':', ':'),\n ('hydrochoerus', 'NN'),\n ('hydrochaeris', 'NN'),\n (')', ')'),\n ('é', 'VBZ'),\n ('uma', 'JJ'),\n ('espécie', 'FW'),\n ('de', 'FW'),\n ('mamífero', 'FW'),\n ('roedor', 'NN'),\n ('da', 'NN'),\n ('família', 'NN'),\n ('caviidae', 'NN'),\n ('e', 'NN'),\n ('subfamília', 'JJ'),\n ('hydrochoerinae', 'NN'),\n ('.', '.'),\n ('alguns', 'JJ'),\n ('autores', 'NNS'),\n ('consideram', 'VBP'),\n ('que', 'JJ'),\n ('deva', 'NN'),\n ('ser', 'NN'),\n ('classificada', 'NN'),\n ('em', 'NN'),\n ('uma', 'JJ'),\n ('família', 'JJ'),\n ('própria', 'NN'),\n ('.', '.'),\n ('está', 'CC'),\n ('incluída', 'JJ'),\n ('no', 'DT'),\n ('mesmo', 'JJ'),\n ('grupo', 'NN'),\n ('de', 'IN'),\n ('roedores', 'NNS'),\n ('ao', 'VBP'),\n ('qual', 'JJ'),\n ('se', 'NN'),\n ('classificam', 'NN'),\n ('as', 'IN'),\n ('pacas', 'NN'),\n (',', ','),\n ('cutias', 'NN'),\n (',', ','),\n ('os', 'JJ'),\n ('preás', 'NN'),\n ('e', 'NN'),\n ('o', 'IN'),\n ('porquinho-da-índia', 'NN'),\n ('.', '.'),\n ('ocorre', 'JJ'),\n ('por', 'NN'),\n ('toda', 'VBD'),\n ('a', 'DT'),\n ('américa', 'NN'),\n ('do', 'VBP'),\n ('sul', 'VB'),\n ('ao', 'VB'),\n ('leste', 'NN'),\n ('dos', 'NN'),\n ('andes', 'NNS'),\n ('em', 'VBP'),\n ('habitats', 'NNS'),\n ('associados', 'VBP'),\n ('a', 'DT'),\n ('rios', 'NN'),\n (',', ','),\n ('lagos', 'JJ'),\n ('e', 'NN'),\n ('pântanos', 'NNS'),\n (',', ','),\n ('do', 'VBP'),\n ('nível', 'RB'),\n ('do', 'VB'),\n ('mar', 'JJR'),\n ('até', 'VB'),\n ('1', 'CD'),\n ('300', 'CD'),\n ('m', 'NN'),\n ('de', 'FW'),\n ('altitude', 'NN'),\n ('.', '.'),\n ('extremamente', 'CC'),\n ('adaptável', 'NN'),\n (',', ','),\n ('pode', 'NN'),\n ('ocorrer', 'NN'),\n ('em', 'NN'),\n ('ambientes', 'VBZ'),\n ('altamente', 'JJ'),\n ('alterados', 'NNS'),\n ('pelo', 'VBP'),\n ('ser', 'JJR'),\n ('humano', 'NN'),\n ('.', '.'),\n ('é', 'CC'),\n ('o', 'JJ'),\n ('maior', 'JJ'),\n ('roedor', 'NN'),\n ('do', 'VBP'),\n ('mundo', 'NNS'),\n (',', ','),\n ('pesando', 'NN'),\n ('até', 'VBD'),\n ('91', 'CD'),\n ('kg', 'NN'),\n ('e', 'NN'),\n ('medindo', 'NN'),\n ('até', 'VBD'),\n ('1,2', 'CD'),\n ('m', 'NN'),\n ('de', 'FW'),\n ('comprimento', 'NN'),\n ('e', 'NN'),\n ('60', 'CD'),\n ('cm', 'NN'),\n ('de', 'FW'),\n ('altura', 'NN'),\n ('.', '.'),\n ('a', 'DT'),\n ('pelagem', 'NN'),\n ('é', 'NN'),\n ('densa', 'NN'),\n (',', ','),\n ('de', 'FW'),\n ('cor', 'FW'),\n ('avermelhada', 'FW'),\n ('a', 'DT'),\n ('marrom', 'NN'),\n ('escuro', 'NN'),\n ('.', '.'),\n ('é', 'CC'),\n ('possível', 'JJ'),\n ('distinguir', 'NN'),\n ('os', 'NN'),\n ('machos', 'NN'),\n ('por', 'NN'),\n ('conta', 'NN'),\n ('da', 'NN'),\n ('presença', 'NN'),\n ('de', 'IN'),\n ('uma', 'JJ'),\n ('glândula', 'NN'),\n ('proeminente', 'VBZ'),\n ('no', 'DT'),\n ('focinho', 'NN'),\n ('apesar', 'RB'),\n ('do', 'VBP'),\n ('dimorfismo', 'VB'),\n ('sexual', 'JJ'),\n ('não', 'NN'),\n ('ser', 'NN'),\n ('aparente', 'NN'),\n ('.', '.'),\n ('existe', 'NN'),\n ('uma', 'JJ'),\n ('série', 'NN'),\n ('de', 'IN'),\n ('adaptações', 'FW'),\n ('no', 'DT'),\n ('sistema', 'NN'),\n ('digestório', 'NN'),\n ('à', 'NNP'),\n ('herbivoria', 'NN'),\n (',', ','),\n ('principalmente', 'VBZ'),\n ('no', 'DT'),\n ('ceco', 'NN'),\n ('.', '.'),\n ('alcança', 'VB'),\n ('a', 'DT'),\n ('maturidade', 'NN'),\n ('sexual', 'JJ'),\n ('com', 'NN'),\n ('cerca', 'NN'),\n ('de', 'IN'),\n ('1,5', 'CD'),\n ('ano', 'NN'),\n ('de', 'FW'),\n ('idade', 'NN'),\n (',', ','),\n ('e', 'CC'),\n ('as', 'IN'),\n ('fêmeas', 'JJ'),\n ('dão', 'NN'),\n ('à', 'NNP'),\n ('luz', 'VBZ'),\n ('geralmente', 'VB'),\n ('a', 'DT'),\n ('quatro', 'NN'),\n ('filhotes', 'VBZ'),\n ('por', 'JJ'),\n ('vez', 'NN'),\n (',', ','),\n ('pesando', 'NN'),\n ('até', 'VBD'),\n ('1,5', 'CD'),\n ('kg', 'NN'),\n ('e', 'NN'),\n ('já', 'NN'),\n ('nascem', 'JJ'),\n ('com', 'NN'),\n ('pelos', 'NN'),\n ('e', 'NN'),\n ('dentição', 'NN'),\n ('permanente', 'NN'),\n ('.', '.'),\n ('em', 'NN'),\n ('cativeiro', 'NN'),\n (',', ','),\n ('pode', 'NN'),\n ('viver', 'NN'),\n ('até', 'VBZ'),\n ('12', 'CD'),\n ('anos', 'NN'),\n ('de', 'FW'),\n ('idade', 'NN'),\n ('.', '.')]"
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"textos/capivara-pt.txt\", 'rb') as f:\n",
    "    text = f.read()\n",
    "\n",
    "nltk.pos_tag(tokenizar(text.decode('utf8')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:59:16.347619400Z",
     "start_time": "2023-09-16T22:59:16.273030700Z"
    }
   },
   "id": "3c21416f8587b057"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Normalização e Limpeza"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "459d69f2030e2a1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A) Lower casing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aa433444d26aef0"
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "def tudo_minisculo(text):\n",
    "    return text.lower()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:22:20.606648Z",
     "start_time": "2023-09-16T22:22:20.525737400Z"
    }
   },
   "id": "1b55f9dc1d5540e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "B) Remoção de pontuação e outro caracteres especiais"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0148e6c1cd7715c"
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "def remocao_caracteres_especiais(text):\n",
    "    padrao = r'[^\\w\\s]'\n",
    "    return re.sub(padrao, '', text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:22:20.649042900Z",
     "start_time": "2023-09-16T22:22:20.579650600Z"
    }
   },
   "id": "b11065843fe62fbc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "C) Remoção de stopwords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d866bf150bd5dfa"
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "def remover_stopwords(texto):\n",
    "    # Carregue a lista de stopwords da língua desejada (exemplo: português)\n",
    "    stopwords_lista = nltk.corpus.stopwords.words('portuguese')\n",
    "    \n",
    "    # Tokenize o texto em palavras\n",
    "    palavras = word_tokenize(texto)\n",
    "    \n",
    "    # Crie uma lista de palavras sem as stopwords\n",
    "    palavras_sem_stopwords = [palavra for palavra in palavras if palavra.lower() not in stopwords_lista]\n",
    "    \n",
    "    # Recrie o texto sem as stopwords\n",
    "    texto_sem_stopwords = ' '.join(palavras_sem_stopwords)\n",
    "    \n",
    "    return texto_sem_stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:22:20.702726800Z",
     "start_time": "2023-09-16T22:22:20.649042900Z"
    }
   },
   "id": "9586a3cd1ea0a4e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "D) Remoção das palavras frequentes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c49c6f6c4168c8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1 - Remoção das N palavras mais frequentes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4299aea1a89d696"
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "def remocao_maior_recorrencia(text):\n",
    "    tokens = tokenizar(text)\n",
    "    cont = 0\n",
    "    maior = 0\n",
    "    primeira_vez = True\n",
    "    aux_tokens = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        for j in tokens :\n",
    "            if i == j:\n",
    "                cont+=1\n",
    "                \n",
    "        if primeira_vez:\n",
    "            maior = cont\n",
    "            primeira_vez = False\n",
    "        \n",
    "\n",
    "        if cont >= maior:\n",
    "            maior = cont\n",
    "        else:\n",
    "            aux_tokens.append(i)\n",
    "        cont = 0\n",
    "\n",
    "    \n",
    "    return aux_tokens\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:22:20.800006200Z",
     "start_time": "2023-09-16T22:22:20.751548200Z"
    }
   },
   "id": "81fa7e1eecf1ee4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 - Remoção das palavras que ocorrem mais que N vezes, sendo N um argumento da função"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4292ab1756541b20"
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "def remocao_recorrencia(text, recorencia_max):\n",
    "    tokens = tokenizar(text)\n",
    "    cont = 0\n",
    "    lista_com_recorrencia_removida = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        for j in tokens :\n",
    "            if i == j:\n",
    "                cont+=1\n",
    "            \n",
    "        if not cont >= recorencia_max :\n",
    "            lista_com_recorrencia_removida.append(i)\n",
    "        cont = 0\n",
    "    \n",
    "    return lista_com_recorrencia_removida\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:22:20.848392200Z",
     "start_time": "2023-09-16T22:22:20.800006200Z"
    }
   },
   "id": "a04ffa3983737abf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "E) Remoção das palavras mais raras"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4449e6d0d35fe79b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1 -  Implemente a remoção das n palavras menos frequentes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c466451aa776b18"
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "def remocao_mais_raros(text):\n",
    "    tokens = tokenizar(text)\n",
    "    cont = 0\n",
    "    menor = 0\n",
    "    primeira_vez = True\n",
    "    aux_tokens = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        for j in tokens :\n",
    "            if i == j:\n",
    "                cont+=1\n",
    "                \n",
    "        if primeira_vez:\n",
    "            menor = cont\n",
    "            primeira_vez = False\n",
    "        \n",
    "\n",
    "        if cont <= menor:\n",
    "            menor = cont\n",
    "        else:\n",
    "            aux_tokens.append(i)\n",
    "        cont = 0\n",
    "\n",
    "    \n",
    "    return aux_tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:22:20.896452Z",
     "start_time": "2023-09-16T22:22:20.848392200Z"
    }
   },
   "id": "e0c735d695e9bb87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 - remoção das palavras que ocorreram menos que n vezes, sendo n um argumento para a função"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a3e6a16dd1f4276"
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "def remocao_raros(text, recorencia_max):\n",
    "    tokens = tokenizar(text)\n",
    "    cont = 0\n",
    "    lista_raros = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        for j in tokens :\n",
    "            if i == j:\n",
    "                cont+=1\n",
    "        \n",
    "        if not cont <= recorencia_max :\n",
    "            lista_raros.append(i)\n",
    "        cont = 0\n",
    "    \n",
    "    return lista_raros\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:22:20.946064800Z",
     "start_time": "2023-09-16T22:22:20.896452Z"
    }
   },
   "id": "dc62520078ae2fa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conte  a quantidade de types e tokens no  seu texto"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e7e9ce8aa38c6fe"
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "def quantidade_type(text):\n",
    "    return len(set(text))\n",
    "    \n",
    "def quantidade_token(text):\n",
    "    return len(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:26:18.835664200Z",
     "start_time": "2023-09-16T22:26:18.830622200Z"
    }
   },
   "id": "1f4d3277ddd34ff7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessamento"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71b8a45b02ddfe15"
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "with open(\"textos/capivara-pt.txt\", 'rb') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.decode('utf8')    \n",
    "\n",
    "#A\n",
    "text = tudo_minisculo(text)\n",
    "#B\n",
    "text = remocao_caracteres_especiais(text)\n",
    "#C\n",
    "text = remover_stopwords(text)\n",
    "#D\n",
    "#text = remocao_maior_recorrencia(text)\n",
    "\n",
    "text = tokenizar(text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:23:06.656878400Z",
     "start_time": "2023-09-16T22:23:06.649880600Z"
    }
   },
   "id": "cc7004bb372e0284"
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de type 107\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de type\",quantidade_type(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:28:51.576586600Z",
     "start_time": "2023-09-16T22:28:51.473392900Z"
    }
   },
   "id": "9dd44b74cc05c8c4"
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de tokens 117\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de tokens\",quantidade_token(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:29:11.609731600Z",
     "start_time": "2023-09-16T22:29:11.599184400Z"
    }
   },
   "id": "7aa20018f1e2a4ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Faça um ranking dos 20 types mais frequentes no texto, mostrando também a frequência de cada um deles;"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e16e389271591dde"
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking dos 20 tipos mais frequentes no texto:\n",
      "Tipo: roedor, Frequência: 2\n",
      "Tipo: família, Frequência: 2\n",
      "Tipo: m, Frequência: 2\n",
      "Tipo: pode, Frequência: 2\n",
      "Tipo: pesando, Frequência: 2\n",
      "Tipo: kg, Frequência: 2\n",
      "Tipo: 12, Frequência: 2\n",
      "Tipo: sexual, Frequência: 2\n",
      "Tipo: 15, Frequência: 2\n",
      "Tipo: idade, Frequência: 2\n",
      "Tipo: capivara, Frequência: 1\n",
      "Tipo: nome, Frequência: 1\n",
      "Tipo: científico, Frequência: 1\n",
      "Tipo: hydrochoerus, Frequência: 1\n",
      "Tipo: hydrochaeris, Frequência: 1\n",
      "Tipo: espécie, Frequência: 1\n",
      "Tipo: mamífero, Frequência: 1\n",
      "Tipo: caviidae, Frequência: 1\n",
      "Tipo: subfamília, Frequência: 1\n",
      "Tipo: hydrochoerinae, Frequência: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def contar_tipos_e_tokens(texto):\n",
    "    \n",
    "    contador_tipos = Counter(texto)\n",
    "    ranking = contador_tipos.most_common(20)\n",
    "\n",
    "    return ranking\n",
    "\n",
    "ranking = contar_tipos_e_tokens(text)\n",
    "\n",
    "# Exibir o ranking dos 20 tipos mais frequentes\n",
    "print(\"Ranking dos 20 tipos mais frequentes no texto:\")\n",
    "for tipo, frequencia in ranking:\n",
    "    print(f\"Tipo: {tipo}, Frequência: {frequencia}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:41:17.876025300Z",
     "start_time": "2023-09-16T22:41:17.866960700Z"
    }
   },
   "id": "76dc49c4fef47093"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a7b4a480dcce26c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
