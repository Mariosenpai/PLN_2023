{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "import nltk\n",
        "from nltk.lm.models import Lidstone\n",
        "from nltk.lm.models import Laplace\n",
        "from nltk.corpus import machado\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "import re\n",
        "\n",
        "from nltk.corpus import machado\n",
        "import nltk"
      ],
      "metadata": {
        "id": "3ZCo1_t9Fr8_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções"
      ],
      "metadata": {
        "id": "-28Cdj89D7DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_stopwords(texto):\n",
        "    # Carregue a lista de stopwords da língua desejada (exemplo: português)\n",
        "    stopwords_lista = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "    # Tokenize o texto em palavras\n",
        "    palavras = word_tokenize(texto)\n",
        "\n",
        "    # Crie uma lista de palavras sem as stopwords\n",
        "    palavras_sem_stopwords = [palavra for palavra in palavras if palavra.lower() not in stopwords_lista]\n",
        "\n",
        "    # Recrie o texto sem as stopwords\n",
        "    texto_sem_stopwords = ' '.join(palavras_sem_stopwords)\n",
        "\n",
        "    return texto_sem_stopwords\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    bigrams = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        bigram =  \" \".join(words[i:i+n])\n",
        "        bigrams.append(tuple(bigram.split()))\n",
        "\n",
        "    return bigrams\n",
        "\n",
        "def remocao_caracteres_especiais(text):\n",
        "    padrao = r'[^\\w]+'\n",
        "    return re.sub(padrao, ' ', text)\n",
        "\n",
        "# Função para gerar texto\n",
        "def generate_text(seed_text, next_words, model, max_sequence_length):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_word_index = np.argmax(predicted)\n",
        "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text\n",
        "\n",
        "def calculate_perplexity(model, test_input, tokenizer, max_sequence_length):\n",
        "    token_list = tokenizer.texts_to_sequences([test_input])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted)\n",
        "    predicted_word_prob = predicted[0][predicted_word_index]\n",
        "    perplexity = 1 / predicted_word_prob\n",
        "    return perplexity\n",
        "\n",
        "def criar_modelo_n_gram(text,n, model=None, gamma=0.1):\n",
        "    tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
        "                  for sent in sent_tokenize(text)]\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "    if model == 'li':\n",
        "        model_machado = Lidstone(gamma,order=n )\n",
        "    elif model == 'la':\n",
        "        model_machado = Laplace(order=n)\n",
        "    else:\n",
        "        model_machado = MLE(n)\n",
        "\n",
        "    model_machado.fit(train_data, padded_sents)\n",
        "    return model_machado\n",
        "\n",
        "def trans_corpus_text(corpus):\n",
        "    text = ''\n",
        "    for i in corpus:\n",
        "        text += machado.raw(i) +' '\n",
        "\n",
        "    #Pre-processamento\n",
        "    #text = remover_stopwords(text)\n",
        "    text = remocao_caracteres_especiais(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def model_rnn():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(total_words, 64, input_length=max_sequence_length - 1))\n",
        "  model.add(tf.keras.layers.SimpleRNN(128))\n",
        "  model.add(tf.keras.layers.Dense(total_words, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "Lg5uf9pdFs5J"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTumzcuj3j0p",
        "outputId": "f60c33ec-7349-45c5-b55d-c275b176cf48"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('machado')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqoGntY0OnMU",
        "outputId": "9167bff8-f4af-4192-defb-0b8cfe46ff6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package machado to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus"
      ],
      "metadata": {
        "id": "B_5JdhffD_QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = machado.fileids()\n",
        "\n",
        "text = ''\n",
        "cont = 0\n",
        "total_contos = 1\n",
        "for i in corpus:\n",
        "    text += machado.raw(i) +' '\n",
        "    if cont == total_contos:\n",
        "        break\n",
        "    cont+=1"
      ],
      "metadata": {
        "id": "kxrjHj70GDnt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N Grams"
      ],
      "metadata": {
        "id": "S-C6U02zEB9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_n_gram = criar_modelo_n_gram(text,4)"
      ],
      "metadata": {
        "id": "ezG8vXmx2-0a"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testando a geração de frases"
      ],
      "metadata": {
        "id": "mTpXr_0EEsH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en = 'O amor que vem'\n",
        "t = f'{en} '\n",
        "for i in model_n_gram.generate(20, text_seed=en.split()):\n",
        "    t += i + ' '\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky0s5JOtDI7J",
        "outputId": "b185eae1-ec26-4941-db73-8e09a97cae9b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O amor que vem cá , adelaide , que era um espírito fraco , cederia ao último que lhe falasse , e os olhos \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexidade infinita"
      ],
      "metadata": {
        "id": "Rb65_ZU6EHhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_n_gram.perplexity('O amor que vem')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5S5sbfi4R_m",
        "outputId": "c6407cd5-2e60-4866-da7f-85b4b54ec5a7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo RNN"
      ],
      "metadata": {
        "id": "Dv-UNmiaEKpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4XZNhNuEvGX",
        "outputId": "4c998405-395f-474d-96e4-2a612b7cd76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2785/2785 [==============================] - 93s 32ms/step - loss: 6.7921\n",
            "Epoch 2/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 6.0373\n",
            "Epoch 3/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 5.5412\n",
            "Epoch 4/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 5.0991\n",
            "Epoch 5/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 4.6935\n",
            "Epoch 6/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 4.3109\n",
            "Epoch 7/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 3.9573\n",
            "Epoch 8/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 3.6310\n",
            "Epoch 9/100\n",
            "2785/2785 [==============================] - 74s 26ms/step - loss: 3.3377\n",
            "Epoch 10/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 3.0865\n",
            "Epoch 11/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 2.8622\n",
            "Epoch 12/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 2.6722\n",
            "Epoch 13/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 2.5100\n",
            "Epoch 14/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 2.3644\n",
            "Epoch 15/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 2.2381\n",
            "Epoch 16/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 2.1306\n",
            "Epoch 17/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 2.0328\n",
            "Epoch 18/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.9478\n",
            "Epoch 19/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.8740\n",
            "Epoch 20/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.8015\n",
            "Epoch 21/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.7455\n",
            "Epoch 22/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 1.6894\n",
            "Epoch 23/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.6679\n",
            "Epoch 24/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.5889\n",
            "Epoch 25/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.5492\n",
            "Epoch 26/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 1.5096\n",
            "Epoch 27/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.4733\n",
            "Epoch 28/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.4431\n",
            "Epoch 29/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.4113\n",
            "Epoch 30/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 1.3847\n",
            "Epoch 31/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.3586\n",
            "Epoch 32/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.3360\n",
            "Epoch 33/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.3159\n",
            "Epoch 34/100\n",
            "2785/2785 [==============================] - 74s 26ms/step - loss: 1.2903\n",
            "Epoch 35/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.2757\n",
            "Epoch 36/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.2580\n",
            "Epoch 37/100\n",
            "2785/2785 [==============================] - 78s 28ms/step - loss: 1.6960\n",
            "Epoch 38/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.3299\n",
            "Epoch 39/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.2513\n",
            "Epoch 40/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.2443\n",
            "Epoch 41/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.2155\n",
            "Epoch 42/100\n",
            "2785/2785 [==============================] - 79s 28ms/step - loss: 1.1982\n",
            "Epoch 43/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.1835\n",
            "Epoch 44/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.1653\n",
            "Epoch 45/100\n",
            "2785/2785 [==============================] - 78s 28ms/step - loss: 1.1551\n",
            "Epoch 46/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.1381\n",
            "Epoch 47/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.1353\n",
            "Epoch 48/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.1140\n",
            "Epoch 49/100\n",
            "2785/2785 [==============================] - 83s 30ms/step - loss: 1.1107\n",
            "Epoch 50/100\n",
            "2785/2785 [==============================] - 79s 28ms/step - loss: 1.1018\n",
            "Epoch 51/100\n",
            "2785/2785 [==============================] - 79s 29ms/step - loss: 1.0934\n",
            "Epoch 52/100\n",
            "2785/2785 [==============================] - 77s 28ms/step - loss: 1.0828\n",
            "Epoch 53/100\n",
            "2785/2785 [==============================] - 77s 27ms/step - loss: 1.7194\n",
            "Epoch 54/100\n",
            "2785/2785 [==============================] - 82s 29ms/step - loss: 1.7305\n",
            "Epoch 55/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.2279\n",
            "Epoch 56/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.1463\n",
            "Epoch 57/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.1269\n",
            "Epoch 58/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.0999\n",
            "Epoch 59/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.0866\n",
            "Epoch 60/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.0744\n",
            "Epoch 61/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.0629\n",
            "Epoch 62/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.0532\n",
            "Epoch 63/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.0472\n",
            "Epoch 64/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 1.0358\n",
            "Epoch 65/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.0338\n",
            "Epoch 66/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 1.0284\n",
            "Epoch 67/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 1.0226\n",
            "Epoch 68/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 1.0189\n",
            "Epoch 69/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 1.0097\n",
            "Epoch 70/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 1.0079\n",
            "Epoch 71/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 1.0049\n",
            "Epoch 72/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 0.9983\n",
            "Epoch 73/100\n",
            "2785/2785 [==============================] - 74s 26ms/step - loss: 0.9936\n",
            "Epoch 74/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 0.9868\n",
            "Epoch 75/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9889\n",
            "Epoch 76/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9807\n",
            "Epoch 77/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9745\n",
            "Epoch 78/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 0.9723\n",
            "Epoch 79/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 0.9701\n",
            "Epoch 80/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 0.9684\n",
            "Epoch 81/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 0.9677\n",
            "Epoch 82/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9596\n",
            "Epoch 83/100\n",
            "2785/2785 [==============================] - 72s 26ms/step - loss: 0.9647\n",
            "Epoch 84/100\n",
            "2785/2785 [==============================] - 72s 26ms/step - loss: 0.9559\n",
            "Epoch 85/100\n",
            "2785/2785 [==============================] - 74s 26ms/step - loss: 0.9604\n",
            "Epoch 86/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 0.9543\n",
            "Epoch 87/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 0.9456\n",
            "Epoch 88/100\n",
            "2785/2785 [==============================] - 72s 26ms/step - loss: 0.9492\n",
            "Epoch 89/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 0.9462\n",
            "Epoch 90/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9473\n",
            "Epoch 91/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9365\n",
            "Epoch 92/100\n",
            "2785/2785 [==============================] - 74s 26ms/step - loss: 0.9478\n",
            "Epoch 93/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9438\n",
            "Epoch 94/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 0.9325\n",
            "Epoch 95/100\n",
            "2785/2785 [==============================] - 74s 26ms/step - loss: 0.9412\n",
            "Epoch 96/100\n",
            "2785/2785 [==============================] - 73s 26ms/step - loss: 0.9409\n",
            "Epoch 97/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 0.9326\n",
            "Epoch 98/100\n",
            "2785/2785 [==============================] - 76s 27ms/step - loss: 0.9384\n",
            "Epoch 99/100\n",
            "2785/2785 [==============================] - 75s 27ms/step - loss: 0.9338\n",
            "Epoch 100/100\n",
            "2785/2785 [==============================] - 74s 27ms/step - loss: 0.9384\n",
            "Aqui está um exemplo a que estava a ânimo de ouvir encontrar o simples satisfação comum de emílio conhecia duas nuvens o ar disse\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Pré-processamento de texto\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Criar sequências de palavras\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Padronizar sequências\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Dividir em dados de entrada e saída\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# Model RNN\n",
        "model = model_rnn()\n",
        "\n",
        "# Treinamento\n",
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testando a geração de frases"
      ],
      "metadata": {
        "id": "qrokCF2LEnlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text(\"O amor que vem\", 20, model, max_sequence_length)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKlbX-dLy18K",
        "outputId": "bdceec6a-de00-481c-962a-b505287f77df"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O amor que vem com este sujeito de quem seria a viúva não sei mas de não é nada menos que ele dizia casar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexidade"
      ],
      "metadata": {
        "id": "fQRNM6DeEkOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = \"O amor que vem\"\n",
        "perplexity = calculate_perplexity(model, test_input, tokenizer, max_sequence_length)\n",
        "print(f\"Perplexidade: {perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnDLeaeezvvc",
        "outputId": "c8defcdc-3784-4a82-ea2d-21c511717124"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexidade: 1.28\n"
          ]
        }
      ]
    }
  ]
}